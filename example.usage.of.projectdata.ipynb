{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ProjectData import ProjectData\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the cell below, we import the data, add custom features, hash the text and create numpy matrices from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "\n",
    "###############################\n",
    "# Init our data wrapper object\n",
    "###############################\n",
    "projd = ProjectData('../train.json','../test.json')\n",
    "\n",
    "print 'Loading ProjectData took :' + str(time.time() - stime) + ' seconds.'\n",
    "\n",
    "##################\n",
    "# Data options\n",
    "###################\n",
    "\n",
    "# go to log price? Default = True\n",
    "projd.log_price = True\n",
    "\n",
    "# features to include. \n",
    "# By default, ProjectData includes all hand-crafted features.\n",
    "#self.dense_matrix_columns = [  'feature1', 'feature2', 'etc'  ]\n",
    "\n",
    "# how many neighborhood clusters should we use? Default = 50\n",
    "projd.num_neighborhood_clusters = 100\n",
    "\n",
    "# Do tfidf analysis? \n",
    "# Default = False\n",
    "projd.tfidf = False\n",
    "\n",
    "# how many slots for text hashing?\n",
    "# Default = -1 i.e. no hashing \n",
    "projd.feature_hash_n = 1000\n",
    "\n",
    "###################\n",
    "# Process the data\n",
    "###################\n",
    "\n",
    "stime = time.time()\n",
    "\n",
    "# create handcrafted features\n",
    "projd.add_handcrafted_features()\n",
    "\n",
    "# has the description text\n",
    "projd.add_text_features()\n",
    "\n",
    "# build the (sparse) matrices\n",
    "projd.build_matrices()\n",
    "print 'Data processing took: ' + str(time.time() - stime) + ' seconds.'\n",
    "\n",
    "# To speed things up, if projd.features_hash_n is small enough (for your computer's memory) \n",
    "# then we can safely make our data matrices dense by running:\n",
    "\n",
    "projd.make_training_matrix_dense()\n",
    "projd.make_test_matrix_dense()\n",
    "\n",
    "# To return them to a sparse representation, we can run:\n",
    "projd.make_training_matrix_sparse()\n",
    "projd.make_test_matrix_sparse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In the cell below, we describe the relavent instance variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '###################\\n#Training Data\\n###################'\n",
    "print 'Training data             : projd.train_data ('  + str(type(projd.train_data))   + ')'\n",
    "print 'Number of training entries: projd.train_n    ('  + str(type(projd.train_n))      + ')'\n",
    "print 'Training data matrix      : projd.train_matrix ('+ str(type(projd.train_matrix)) + ')'\n",
    "print 'Training data labels      : projd.train_labels ('+ str(type(projd.train_labels)) + ')'\n",
    "print ''\n",
    "print '###################\\n#Test Data\\n###################'\n",
    "print 'Test data                 : projd.test_data ('  + str(type(projd.train_data))  + ')'\n",
    "print 'Number of training entries: projd.train_n   ('  + str(type(projd.train_n))     + ')'\n",
    "print 'Test data matrix          : projd.test_matrix ('+ str(type(projd.test_matrix)) + ')'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we train a Gradient Boosting Classifier (GBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GBCclf = GradientBoostingClassifier()\n",
    "\n",
    "stime  = time.time()\n",
    "\n",
    "projd.make_training_matrix_dense()\n",
    "GBCclf.fit(projd.train_matrix,projd.train_labels)\n",
    "projd.make_training_matrix_sparse()\n",
    "\n",
    "print \"Training took: \" + str(time.time() - stime) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, we use our trained GBC to make predictions on our test set\n",
    "\n",
    "### Note the use of the ProjectData instance method \"sparse_prediction_proba\". GBCclf.predict_proba requires dense numpy arrays as input, while our matrix is sparse. To get around this, ProjectData.sparse_prediction_proba converts batches of the test data to dense matrices. The batchsize is the number of rows of the test data that get converted to dense arrays per iteration. If, for example, your laptop has less memory available then you should decrease the \"batchsize\" argument. \"verbose = True\" just outputs progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mapping of columns to interest levels\n",
    "classes = GBCclf.classes_\n",
    "\n",
    "# make predictions on our test set.\n",
    "preds = projd.sparse_prediction_proba(GBCclf.predict_proba, batchsize = 10000, verbose = True)\n",
    "\n",
    "# output the predictions to \"gbc_neighborhood.csv\"\n",
    "projd.kaggle_output_from_probs(preds,classes,'gbc_neighborhood.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below, we show how to use ProjectData to make the test set a holdout set from the training data, train a model, and compute the multi-class loss on the holdout set. \n",
    "\n",
    "## The test set now becomes a subset of the training set, and there is a new instance variable called self.test_labels. \n",
    "\n",
    "## We also need to shuffle the dataset, because by default it just partitions the training set by drawing a line \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stime = time.time()\n",
    "\n",
    "###############################\n",
    "# Init our data wrapper object\n",
    "###############################\n",
    "projd = ProjectData('../train.json',\n",
    "                    '../test.json',\n",
    "                    holdout_set = True,       # HERE IS WHERE THE MAGIC HAPPENS\n",
    "                    holdout_set_ratio = 0.2)  # Let the test set be 0.2 * \"size of training set\"\n",
    "\n",
    "print 'Loading ProjectData took : ' + str(time.time() - stime) + ' seconds.'\n",
    "\n",
    "##################\n",
    "# Data options\n",
    "###################\n",
    "\n",
    "projd.num_neighborhood_clusters = 90\n",
    "projd.feature_hash_n = 1000\n",
    "\n",
    "\n",
    "###################\n",
    "# Process the data\n",
    "###################\n",
    "\n",
    "stime = time.time()\n",
    "\n",
    "# create handcrafted features\n",
    "projd.add_handcrafted_features()\n",
    "\n",
    "# has the description text\n",
    "#projd.add_text_features()\n",
    "\n",
    "# build the (sparse) matrices\n",
    "projd.build_matrices()\n",
    "print 'Data processing took     : ' + str(time.time() - stime) + ' seconds.'\n",
    "\n",
    "# To speed things up, if projd.features_hash_n is small enough (for your computer's memory) \n",
    "# then we can safely make our data matrices dense by running:\n",
    "\n",
    "\n",
    "# shuffle the dataset\n",
    "projd.shuffle_matrices(seed = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_X = projd.train_matrix[projd.train_labels==0]\n",
    "low_num=len(low_X)\n",
    "\n",
    "# Changing to 1's if medium interest, 0 otherwise for the purpose of oversampling\n",
    "newlabels_med=(projd.train_labels==1).astype(int)\n",
    "sm_med = SMOTE(ratio=0.45)\n",
    "sm_high = SMOTE(ratio=0.16)\n",
    "\n",
    "med_X, med_Y=sm_med.fit_sample(projd.train_matrix, newlabels_med)\n",
    "med_X=med_X[med_Y==1]\n",
    "med_num=len(med_X)\n",
    "\n",
    "# New labels for high interest\n",
    "newlabels_high=(projd.train_labels==2).astype(int)\n",
    "high_X, high_Y=sm_high.fit_sample(projd.train_matrix, newlabels_high)\n",
    "high_X=high_X[high_Y==1]\n",
    "high_num=len(high_X)\n",
    "\n",
    "# Concatenate\n",
    "new_data=np.concatenate((low_X,med_X,high_X),axis=0)\n",
    "\n",
    "# Make new labels\n",
    "x0=[0]*low_num\n",
    "x0=np.array(x0)\n",
    "x1=[1]*med_num\n",
    "x1=np.array(x1)\n",
    "x2=[2]*high_num\n",
    "x2=np.array(x2)\n",
    "new_labs=np.concatenate((x0,x1,x2),axis=0)\n",
    "#print new_labs\n",
    "\n",
    "# Shuffle\n",
    "permutation = np.random.permutation(new_labs.shape[0])\n",
    "shuff_data=new_data[permutation]\n",
    "shuff_labels=new_labs[permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GBCclf = GradientBoostingClassifier()\n",
    "\n",
    "stime  = time.time()\n",
    "GBCclf.fit(shuff_data,shuff_labels)\n",
    "\n",
    "print \"Training took: \" + str(time.time() - stime) + ' seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the log-loss on the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = GBCclf.predict_proba(projd.test_matrix)\n",
    "print projd.get_log_loss(y,GBCclf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
